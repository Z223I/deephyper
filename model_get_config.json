{
"name": "model_1",
"layers":
    [



{"class_name": "InputLayer",
        "config": {"batch_input_shape": [null, 1690],
        "dtype": "float32",
        "sparse": false,
        "ragged": false,
        "name": "input_0"},
    "name": "input_0",
    "inbound_nodes": []},

## forward(x)
## input_0 = x




{"class_name": "Dense",
"config": {
"name": "dense",
"trainable": true,
"dtype": "float32",
"units": 80,
"activation": "linear",
"use_bias": true,
"kernel_initializer":

{"class_name": "GlorotUniform",
"config": {"seed": null}},
"bias_initializer":

{"class_name": "Zeros",
"config": {}},
"kernel_regularizer": null,
"bias_regularizer": null,
"activity_regularizer": null,
"kernel_constraint": null,
"bias_constraint": null},
"name": "dense",
"inbound_nodes": [[["input_0", 0, 0, {}]]]},

self.layer_norm = nn.LayerNorm(input_shape).to(device, dtype=dtype)
##input_0 = self.layer_norm(input_0)

#if batchSamples >= 16:
in_features = numInputs
out_features = 80
self.dense = nn.Linear(in_features, out_features).to(device, dtype=dtype)
##dense = self.dense(input_0)
self.dropout_1 = nn.Dropout2d(dropout1).to(device, dtype=dtype)
##dense = self.dropout_1(dense)




{"class_name": "Activation",
"config": {
"name": "activation",
"trainable": true,
"dtype": "float32",
"activation": "sigmoid"},

"name": "activation",
"inbound_nodes": [[["dense", 0, 0, {}]]]},

self.activation = torch.nn.Sigmoid().to(device, dtype=dtype)
##activation = self.activation(dense)



{"class_name": "Dense",
"config": {
"name": "dense_1",
"trainable": true,
"dtype": "float32",
"units": 80,
"activation": "linear",
"use_bias": true,
"kernel_initializer":
{"class_name": "GlorotUniform",
"config": {"seed": null}},
"bias_initializer":
{"class_name": "Zeros",
"config": {}},
"kernel_regularizer": null,
"bias_regularizer": null,
"activity_regularizer": null,
"kernel_constraint": null,
"bias_constraint": null},

"name": "dense_1",
"inbound_nodes": [[["input_0", 0, 0, {}]]]},

#if batchSamples >= 12:
in_features = out_features
out_features = 80
self.dense_1 = nn.Linear(in_features, out_features).to(device, dtype=dtype)
    ##input_0 = activation
##dense_1 = self.dense_1(input_0)
self.dropout_2 = nn.Dropout2d(dropout2).to(device, dtype=dtype)
##dense_1 = self.dropout_2(dense_1)






{"class_name": "Add",
"config": {
"name": "add",
"trainable": true,
"dtype": "float32"},

"name": "add",
"inbound_nodes": [[["activation", 0, 0, {}], ["dense_1", 0, 0, {}]]]},

# Add
out_features = 1
self.add = torch.nn.Sum().to(device, dtype=dtype)
##add = self.add(activation, dense_1)





{"class_name": "Activation",
"config": {
"name": "activation_1",
"trainable": true,
"dtype": "float32",
"activation": "relu"},

"name": "activation_1",
"inbound_nodes": [[["add", 0, 0, {}]]]},

out_features = 1
self.activation_1 = torch.nn.ReLU().to(device, dtype=dtype)
## Can use ReLU(inplace=False)
##activation_1 = self.activation_1(add)





{"class_name": "Dense",
"config": {
"name": "dense_2",
"trainable": true,
"dtype": "float32",
"units": 96,
"activation": "linear",
"use_bias": true,
"kernel_initializer":

{"class_name": "GlorotUniform",
"config": {"seed": null}},
"bias_initializer":

{"class_name": "Zeros",
"config": {}},
"kernel_regularizer": null,
"bias_regularizer": null,
"activity_regularizer": null,
"kernel_constraint": null,
"bias_constraint": null},

"name": "dense_2",
"inbound_nodes": [[["activation_1", 0, 0, {}]]]},

#if batchSamples >= 10:
in_features = out_features
out_features = 96
self.dense_2 = nn.Linear(in_features, out_features).to(device, dtype=dtype)
##dense_2 = self.dense_2(activation_1)
self.dropout_3 = nn.Dropout2d(dropout2).to(device, dtype=dtype)
##dense_2 = self.dropout_3(dense_2)





{"class_name": "Activation",
"config": {
"name": "activation_2",
"trainable": true,
"dtype": "float32",
"activation": "tanh"},

"name": "activation_2",
"inbound_nodes": [[["dense_2", 0, 0, {}]]]},

out_features = 1
self.activation_2 = torch.nn.Tanh().to(device, dtype=dtype)
##activation_2 = self.activation_2(dense_2)





{"class_name": "Dense",
"config": {
"name": "dense_3",
"trainable": true,
"dtype": "float32",
"units": 96,
"activation": "linear",
"use_bias": true,
"kernel_initializer":


{"class_name": "GlorotUniform",
"config": {"seed": null}},
"bias_initializer":

{"class_name": "Zeros",
"config": {}},
"kernel_regularizer": null,
"bias_regularizer": null,
"activity_regularizer": null,
"kernel_constraint": null,
"bias_constraint": null},

"name": "dense_3",
"inbound_nodes": [[["activation_1", 0, 0, {}]]]},

in_features = out_features
out_features = 96
self.dense_3 = nn.Linear(in_features, out_features).to(device, dtype=dtype)
##dense_3 = self.dense_3(activation_1)
self.dropout_4 = nn.Dropout2d(dropout3).to(device, dtype=dtype)
##dense_3 = self.dropout_4(dense_3)





{"class_name": "Add",
"config": {
"name": "add_1",
"trainable": true,
"dtype": "float32"},

"name": "add_1",
"inbound_nodes": [[["activation_2", 0, 0, {}], ["dense_3", 0, 0, {}]]]},

out_features = 1
self.add_1 = torch.nn.Sum().to(device, dtype=dtype)
##add_1 = self.add_1(activation_2, dense_3)




{"class_name": "Activation",
"config": {
"name": "activation_3",
"trainable": true,
"dtype": "float32",
"activation": "relu"},

"name": "activation_3",
"inbound_nodes": [[["add_1", 0, 0, {}]]]},

out_features = 1
self.activation_3 = torch.nn.ReLU().to(device, dtype=dtype)
## Can use ReLU(inplace=False)
##activation_3 = self.activation_3(add_1)





{"class_name": "Dense",
"config": {
"name": "dense_4",
"trainable": true,
"dtype": "float32",
"units": 80,
"activation": "linear",
"use_bias": true,
"kernel_initializer":

{"class_name": "GlorotUniform",
"config": {"seed": null}},
"bias_initializer":

{"class_name": "Zeros",
"config": {}},
"kernel_regularizer": null,
"bias_regularizer": null,
"activity_regularizer": null,
"kernel_constraint": null,
"bias_constraint": null},

"name": "dense_4",
"inbound_nodes": [[["activation_3", 0, 0, {}]]]},

in_features = out_features
out_features = 80
self.dense_4 = nn.Linear(in_features, out_features).to(device, dtype=dtype)
##dense_4 = self.dense_4(activation_3)
self.dropout_5 = nn.Dropout2d(dropout4).to(device, dtype=dtype)
##dense_4 = self.dropout_5(dense_4)





{"class_name": "Activation",
"config": {
"name": "activation_4",
"trainable": true,
"dtype": "float32",
"activation": "sigmoid"},

"name": "activation_4",
"inbound_nodes": [[["dense_4", 0, 0, {}]]]},

out_features = 1
self.activation_4 = torch.nn.Sigmoid().to(device, dtype=dtype)
activation_4 = self.activation_4(dense_4)





{"class_name": "Dense",
"config": {
"name": "dense_5",
"trainable": true,
"dtype": "float32",
"units": 80,
"activation": "linear",
"use_bias": true,
"kernel_initializer":

{"class_name": "GlorotUniform",
"config": {"seed": null}},
"bias_initializer":

{"class_name": "Zeros",
"config": {}},
"kernel_regularizer": null,
"bias_regularizer": null,
"activity_regularizer": null,
"kernel_constraint": null,
"bias_constraint": null},

"name": "dense_5",
"inbound_nodes": [[["input_0", 0, 0, {}]]]},

in_features = out_features
out_features = 80
self.dense_5 = nn.Linear(in_features, out_features).to(device, dtype=dtype)
##dense_5 = self.dense_5(input_0)
self.dropout_6 = nn.Dropout2d(dropout4).to(device, dtype=dtype)
##dense_5 = self.dropout_6(dense_5)





{"class_name": "Dense",
"config": {
"name": "dense_6",
"trainable": true,
"dtype": "float32",
"units": 80,
"activation": "linear",
"use_bias": true,
"kernel_initializer":

{"class_name": "GlorotUniform",
"config": {"seed": null}},
"bias_initializer":

{"class_name": "Zeros",
"config": {}},
"kernel_regularizer": null,
"bias_regularizer": null,
"activity_regularizer": null,
"kernel_constraint": null,
"bias_constraint": null},

"name": "dense_6",
"inbound_nodes": [[["activation_3", 0, 0, {}]]]},

in_features = out_features
out_features = 80
self.dense_6 = nn.Linear(in_features, out_features).to(device, dtype=dtype)
dense_6 = self.dense_6(activation_3)
self.dropout_7 = nn.Dropout2d(dropout4).to(device, dtype=dtype)
dense_6 = self.dropout_7(dense_6)







{"class_name": "Add",
"config": {
"name": "add_2",
"trainable": true,
"dtype": "float32"},

"name": "add_2",
"inbound_nodes": [[["activation_4", 0, 0, {}], ["dense_5", 0, 0, {}], ["activation_1", 0, 0, {}], ["dense_6", 0, 0, {}]]]},

out_features = 1
self.add_2 = torch.nn.Sum().to(device, dtype=dtype)
##add_2 = self.add_2(activation_4, dense_5, activation_1, dense_6)





{"class_name": "Activation",
"config": {
"name": "activation_5",
"trainable": true,
"dtype": "float32",
"activation": "relu"},

"name": "activation_5",
"inbound_nodes": [[["add_2", 0, 0, {}]]]},

out_features = 1
self.activation_5 = torch.nn.ReLU().to(device, dtype=dtype)
### Can use ReLU(inplace=False)
##activation_5 = self.activation_5(add_2)





{"class_name": "Dense",
"config": {
"name": "dense_7",
"trainable": true,
"dtype": "float32",
"units": 1,
"activation": "linear",
"use_bias": true,
"kernel_initializer":


{"class_name": "GlorotUniform",
"config": {"seed": null}},
"bias_initializer":

{"class_name": "Zeros",
"config": {}},
"kernel_regularizer": null,
"bias_regularizer": null,
"activity_regularizer": null,
"kernel_constraint": null,
"bias_constraint": null},

"name": "dense_7",
"inbound_nodes": [[["activation_5", 0, 0, {}]]]}],

in_features = out_features
out_features = 2 # This is 2 instead of 1 due to the PyTorch softmax.
self.dense_7 = nn.Linear(in_features, out_features).to(device, dtype=dtype)
dense_7 = self.dense_7(activation_5)





"input_layers": [["input_0", 0, 0]],
"output_layers": [["dense_7", 0, 0]]}